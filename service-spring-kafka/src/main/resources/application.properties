spring.application.name=service-spring-kafka
server.port=8080

#kafka
kafka.bootstrap-servers=172.16.0.180:9092,172.16.0.122:9092,172.16.0.131:9092
#=============== producer  =======================
# 写入失败时，重试次数。当leader节点失效，一个repli节点会替代成为leader节点，此时可能出现写入失败，
# 当retris为0时，produce不会重复。retirs重发，此时repli节点完全成为leader节点，不会产生消息丢失。
kafka.producer.retries=0
# 每次批量发送消息的数量,produce积累到一定数据，一次发送
kafka.producer.batch-size=16384
# produce积累数据一次发送，缓存大小达到buffer.memory就发送数据
kafka.producer.buffer-memory=33554432
kafka.producer.linger-ms=5
#=============== consumer  =======================
# 指定默认消费者group id --> 由于在kafka中，同一组中的consumer不会读取到同一个消息，依靠groud.id设置组名
kafka.consumer.group-id=kafka_test
# smallest和largest才有效，如果smallest重新0开始读取，如果是largest从logfile的offset读取。一般情况下我们都是设置smallest
kafka.consumer.auto-offset-reset=earliest
# enable.auto.commit:true --> 设置自动提交offset
kafka.consumer.enable-auto-commit=true
#如果'enable.auto.commit'为true，则消费者偏移自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
kafka.consumer.auto-commit-interval=100
kafka.consumer.max-poll-records=1000